import { GoogleGenerativeAI } from '@google/generative-ai'

// Enhanced Performance Configuration
const API_KEYS = [
  import.meta.env.VITE_GOOGLE_AI_API_KEY,
  import.meta.env.VITE_GOOGLE_AI_API_KEY_2,
  import.meta.env.VITE_GOOGLE_AI_API_KEY_3,
  import.meta.env.VITE_GOOGLE_AI_API_KEY_4,
  import.meta.env.VITE_GOOGLE_AI_API_KEY_5
].filter(key => key && key.trim() !== '')

const MODEL_VERSION = import.meta.env.VITE_AI_MODEL || 'gemini-1.5-flash'

if (API_KEYS.length === 0) {
  console.warn('No Google AI API keys configured. Please set VITE_GOOGLE_AI_API_KEY in your environment.')
}

console.log(`üöÄ Enhanced Performance Mode: ${API_KEYS.length} API key(s) for parallel processing`)
console.log(`ü§ñ Using model: ${MODEL_VERSION}`)

// ‡∏™‡∏£‡πâ‡∏≤‡∏á AI instances ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ API key
const aiInstances = API_KEYS.map(key => new GoogleGenerativeAI(key))

// Performance tracking for each API
interface APIStats {
  requests: number
  errors: number
  lastUsed: number
  avgResponseTime: number
  isHealthy: boolean
  successRate: number
}

const apiStats: APIStats[] = aiInstances.map(() => ({
  requests: 0,
  errors: 0,
  lastUsed: 0,
  avgResponseTime: 0,
  isHealthy: true,
  successRate: 1.0
}))

export interface ProcessingResult {
  success: boolean
  data?: string
  error?: string
  processingTime?: number
  apiUsed?: number
}

export interface ProcessingProgress {
  step: string
  progress: number
  details: string
  logs: string[]
  percentage?: number  // ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging compatibility
  message?: string     // ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging compatibility
}

export class GoogleAIService {
  private progressCallback?: (progress: ProcessingProgress) => void
  private currentLogs: string[] = []

  // Smart API selection based on performance and health
  private getSmartAIInstance(): { model: any; apiIndex: number } {
    // Filter healthy APIs
    const healthyApis = apiStats
      .map((stats, index) => ({ stats, index }))
      .filter(({ stats }) => stats.isHealthy && stats.successRate > 0.3)

    if (healthyApis.length === 0) {
      // Reset all to healthy if all are marked unhealthy
      apiStats.forEach(stats => {
        stats.isHealthy = true
        stats.successRate = Math.max(stats.successRate, 0.5)
      })
      const apiIndex = 0
      return { 
        model: aiInstances[apiIndex].getGenerativeModel({ model: MODEL_VERSION as any }), 
        apiIndex 
      }
    }

    // Calculate score for each API (lower is better)
    const scoredApis = healthyApis.map(({ stats, index }) => {
      const timeSinceLastUse = Date.now() - stats.lastUsed
      const errorWeight = stats.errors * 2
      const requestWeight = stats.requests * 0.1
      const responseTimeWeight = stats.avgResponseTime / 1000
      const healthWeight = stats.isHealthy ? 0 : 10
      const successRateWeight = (1 - stats.successRate) * 5
      
      const score = errorWeight + requestWeight + responseTimeWeight + healthWeight + successRateWeight - (timeSinceLastUse / 10000)
      
      return { index, score, stats }
    })

    // Select API with best score
    const bestApi = scoredApis.reduce((best, current) => 
      current.score < best.score ? current : best
    )

    this.updateProgress('Preparing', 35, '‡πÄ‡∏•‡∏∑‡∏≠‡∏Å API ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î...', 
      `üéØ API #${bestApi.index + 1} (${bestApi.stats.requests} req, ${(bestApi.stats.successRate * 100).toFixed(1)}% success)`)
    
    return { 
      model: aiInstances[bestApi.index].getGenerativeModel({ model: MODEL_VERSION as any }), 
      apiIndex: bestApi.index 
    }
  }

  // Parallel processing for large files
  private async processLargeFileParallel(audioFile: File, type: 'transcribe' | 'summarize'): Promise<string> {
    if (aiInstances.length < 2) {
      return await this.processSingleFile(audioFile, type)
    }

    const fileSizeMB = audioFile.size / (1024 * 1024)
    const optimalChunks = Math.min(Math.ceil(fileSizeMB / 5), aiInstances.length, 4) // Max 4 chunks
    
    this.updateProgress('Processing', 40, `‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏ö‡∏ö Parallel`, 
      `‚ö° Processing large file with ${optimalChunks} parallel requests`)

    const chunkSize = Math.ceil(audioFile.size / optimalChunks)
    const chunks: File[] = []
    
    // Split file into chunks
    for (let i = 0; i < audioFile.size; i += chunkSize) {
      const chunk = audioFile.slice(i, Math.min(i + chunkSize, audioFile.size))
      chunks.push(new File([chunk], `${audioFile.name}.part${chunks.length + 1}`, { type: audioFile.type }))
    }

    // Process chunks in parallel
    const chunkPromises = chunks.map(async (chunk, index) => {
      const startTime = Date.now()
      let retryCount = 0
      const maxRetries = 3
      
      while (retryCount < maxRetries) {
        try {
          const { model, apiIndex } = this.getSmartAIInstance()
          const stats = apiStats[apiIndex]
          
          stats.requests++
          stats.lastUsed = Date.now()
          
          const base64Audio = await this.fileToBase64(chunk)
          const prompt = type === 'transcribe' 
            ? `‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≠‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà`
            : `‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏à‡πâ‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà`

          const result = await model.generateContent([
            prompt,
            {
              inlineData: {
                data: base64Audio,
                mimeType: chunk.type || 'audio/mpeg'
              }
            }
          ])

          const responseText = result.response.text()
          
          // Check for error responses and retry if needed
          if (responseText.includes('‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ') || 
              responseText.includes('‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á') ||
              responseText.includes('‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢')) {
            throw new Error('AI response indicates inability to process audio')
          }

          // Update success stats
          const responseTime = Date.now() - startTime
          stats.avgResponseTime = stats.avgResponseTime === 0 ? responseTime : (stats.avgResponseTime + responseTime) / 2
          stats.successRate = (stats.successRate * stats.requests + 1) / (stats.requests + 1)
          stats.isHealthy = true

          this.updateProgress('Processing', 50 + (index * 30 / chunks.length), 
            `‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà ${index + 1}/${chunks.length}`, 
            `‚úÖ Chunk ${index + 1} completed by API #${apiIndex + 1}`)

          return responseText
        } catch (error: any) {
          retryCount++
          const randomApiIndex = Math.floor(Math.random() * aiInstances.length)
          const stats = apiStats[randomApiIndex]
          stats.errors++
          stats.successRate = Math.max(0, (stats.successRate * stats.requests - 1) / Math.max(stats.requests, 1))
          
          if (error.message?.includes('429') || error.message?.includes('quota')) {
            stats.isHealthy = false
          }
          
          if (retryCount < maxRetries) {
            this.updateProgress('Processing', 50 + (index * 30 / chunks.length), 
              `‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà ${index + 1} (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà ${retryCount + 1})`, 
              `üîÑ Retrying chunk ${index + 1} with different API`)
            await new Promise(resolve => setTimeout(resolve, 1000 * retryCount))
            continue
          }
          
          // If all retries failed, return empty or try different approach
          this.updateProgress('Processing', 50 + (index * 30 / chunks.length), 
            `‡∏Ç‡πâ‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà ${index + 1} ‡πÑ‡∏õ‡∏Å‡πà‡∏≠‡∏ô`, 
            `‚ö†Ô∏è Chunk ${index + 1} failed after ${maxRetries} attempts`)
          
          return '' // Return empty instead of error message
        }
      }
      return ''
    })

    const results = await Promise.all(chunkPromises)

    // Filter out empty results and combine intelligently
    const validResults = results.filter(r => r && r.trim().length > 0)
    
    if (validResults.length === 0) {
      throw new Error('‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á')
    }

    // Combine results seamlessly
    if (type === 'transcribe') {
      return validResults.join(' ') // Join with space for natural flow
    } else {
      return validResults.join('\n\n') // Join with paragraph breaks for summaries
    }
  }

  private async processSingleFile(audioFile: File, type: 'transcribe' | 'summarize'): Promise<string> {
    const startTime = Date.now()
    let retries = Math.min(aiInstances.length * 2, 6) // Max 6 retries
    let lastError: any

    while (retries > 0) {
      try {
        const { model, apiIndex } = this.getSmartAIInstance()
        const stats = apiStats[apiIndex]
        
        stats.requests++
        stats.lastUsed = Date.now()
        
        this.updateProgress('Processing', 60, 
          type === 'transcribe' ? '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ñ‡∏≠‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...' : '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...', 
          `üöÄ API #${apiIndex + 1} processing with ${MODEL_VERSION}`)
        
        const base64Audio = await this.fileToBase64(audioFile)
        const prompt = type === 'transcribe' 
          ? '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ñ‡∏≠‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö'
          : '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ô‡∏µ‡πâ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢'

        const result = await model.generateContent([
          prompt,
          {
            inlineData: {
              data: base64Audio,
              mimeType: audioFile.type || 'audio/mpeg'
            }
          }
        ])

        // Update success stats
        const responseTime = Date.now() - startTime
        stats.avgResponseTime = stats.avgResponseTime === 0 ? responseTime : (stats.avgResponseTime + responseTime) / 2
        stats.successRate = (stats.successRate * stats.requests + 1) / (stats.requests + 1)
        stats.isHealthy = true

        this.updateProgress('Processing', 85, '‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!', 
          `‚úÖ Completed by API #${apiIndex + 1} in ${responseTime}ms`)

        return result.response.text()
      } catch (error: any) {
        retries--
        lastError = error
        
        // Find which API failed and update stats
        const failedApiIndex = Math.floor(Math.random() * aiInstances.length) // Approximate
        const stats = apiStats[failedApiIndex]
        stats.errors++
        stats.successRate = Math.max(0, (stats.successRate * stats.requests - 1) / Math.max(stats.requests, 1))
        
        if (error.message?.includes('429') || error.message?.includes('quota')) {
          stats.isHealthy = false
          this.updateProgress('Processing', 60, 
            `API ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏° ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ API ‡∏≠‡∏∑‡πà‡∏ô (‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ${retries} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á)`, 
            `‚ö†Ô∏è API quota exceeded, switching to another API`)
          
          if (retries > 0) {
            await new Promise(resolve => setTimeout(resolve, Math.min(1000 * (6 - retries), 3000)))
            continue
          }
        } else {
          // For non-quota errors, reduce retry count more aggressively
          retries = Math.max(0, retries - 2)
          if (retries <= 0) {
            throw error
          }
        }
      }
    }

    throw new Error(`‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á: ${lastError?.message}`)
  }

  // Enhanced main processing method
  private async callAI(audioFile: File, type: 'transcribe' | 'summarize'): Promise<string> {
    const fileSizeMB = audioFile.size / (1024 * 1024)
    
    // Decide on processing strategy
    if (fileSizeMB > 8 && aiInstances.length > 1) {
      this.updateProgress('Analyzing', 30, `‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà (${fileSizeMB.toFixed(1)}MB) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö Parallel`, 
        `üìä Large file detected, using parallel processing`)
      return await this.processLargeFileParallel(audioFile, type)
    } else {
      return await this.processSingleFile(audioFile, type)
    }
  }

  setProgressCallback(callback: (progress: ProcessingProgress) => void) {
    this.progressCallback = callback
    this.currentLogs = []
  }

  private updateProgress(step: string, progress: number, details: string, log?: string) {
    if (log) {
      const timestamp = new Date().toLocaleTimeString('th-TH')
      this.currentLogs.push(`${timestamp} - ${log}`)
      // Keep only last 50 logs for better performance
      if (this.currentLogs.length > 50) {
        this.currentLogs = this.currentLogs.slice(-50)
      }
    }
    
    // Always log to console for debugging
    console.log(`[${step}] ${progress}% - ${details}`, log ? `| ${log}` : '')
    
    if (this.progressCallback) {
      this.progressCallback({ 
        step, 
        progress, 
        details, 
        logs: [...this.currentLogs],
        percentage: progress,  // ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging compatibility
        message: details       // ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logging compatibility
      })
    }
  }

  private async fileToBase64(file: File): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader()
      reader.onload = () => {
        const result = reader.result as string
        const base64 = result.split(',')[1]
        resolve(base64)
      }
      reader.onerror = reject
      reader.readAsDataURL(file)
    })
  }

  private simulateDetailedProgress(type: 'transcribe' | 'summarize') {
    return new Promise<void>((resolve) => {
      const steps = type === 'transcribe' 
        ? [
            { progress: 65, detail: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏á...', log: 'üîä ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå spectrum ‡πÄ‡∏™‡∏µ‡∏¢‡∏á' },
            { progress: 70, detail: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏û‡∏π‡∏î...', log: 'üó£Ô∏è ‡πÅ‡∏¢‡∏Å‡πÅ‡∏¢‡∏∞‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î‡∏à‡∏≤‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏£‡∏ö‡∏Å‡∏ß‡∏ô' },
            { progress: 75, detail: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...', log: 'üìù ‡πÉ‡∏ä‡πâ Speech-to-Text AI' },
            { progress: 80, detail: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á...', log: '‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥' }
          ]
        : [
            { progress: 65, detail: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...', log: 'üîç ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç' },
            { progress: 70, detail: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...', log: 'üìÇ ‡∏à‡∏±‡∏î‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤' },
            { progress: 75, detail: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å...', log: 'üí° ‡∏™‡∏Å‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç' },
            { progress: 80, detail: '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ...', log: 'üìã ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢' }
          ]

      let currentStep = 0
      const interval = setInterval(() => {
        if (currentStep < steps.length) {
          const step = steps[currentStep]
          this.updateProgress('Processing', step.progress, step.detail, step.log)
          currentStep++
        } else {
          clearInterval(interval)
          resolve()
        }
      }, 800) // ‡πÅ‡∏ï‡πà‡∏•‡∏∞ step ‡∏´‡πà‡∏≤‡∏á 0.8 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô)
    })
  }

  async transcribeAudio(audioFile: File): Promise<ProcessingResult> {
    const startTime = Date.now()
    
    try {
      this.updateProgress('Starting', 0, '‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ñ‡∏≠‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...', 
        `üéµ File: ${audioFile.name}, Size: ${(audioFile.size / 1024 / 1024).toFixed(2)}MB, Type: ${audioFile.type}`)
      
      this.updateProgress('Validating', 5, '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á...', 
        `üîç Validating audio format and size`)
      
      if (!audioFile.type.startsWith('audio/')) {
        throw new Error('‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô')
      }

      this.updateProgress('Preparing', 15, '‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...', 
        `üìã Available APIs: ${API_KEYS.length}, Selected model: ${MODEL_VERSION}`)

      // Log API health status
      const healthyApis = apiStats.filter(stats => stats.isHealthy).length
      this.updateProgress('Preparing', 20, '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ API...', 
        `üíö Healthy APIs: ${healthyApis}/${API_KEYS.length}`)

      await this.simulateDetailedProgress('transcribe')
      
      const result = await this.callAI(audioFile, 'transcribe')
      
      const processingTime = Date.now() - startTime
      this.updateProgress('Completed', 100, '‡∏ñ‡∏≠‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!', 
        `üéâ Transcription completed in ${processingTime}ms, Result length: ${result.length} characters`)

      return {
        success: true,
        data: result,
        processingTime: processingTime
      }
    } catch (error: any) {
      const processingTime = Date.now() - startTime
      this.updateProgress('Error', 0, '‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î', 
        `‚ùå Error after ${processingTime}ms: ${error.message}`)
      
      return {
        success: false,
        error: error.message
      }
    }
  }

  async summarizeAudio(audioFile: File): Promise<ProcessingResult> {
    const startTime = Date.now()
    
    try {
      this.updateProgress('Starting', 0, '‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...', 
        `üéµ File: ${audioFile.name}, Size: ${(audioFile.size / 1024 / 1024).toFixed(2)}MB, Type: ${audioFile.type}`)
      
      this.updateProgress('Validating', 5, '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á...', 
        `üîç Validating audio format and size`)
      
      if (!audioFile.type.startsWith('audio/')) {
        throw new Error('‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô')
      }

      this.updateProgress('Preparing', 15, '‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...', 
        `üìã Available APIs: ${API_KEYS.length}, Selected model: ${MODEL_VERSION}`)

      // Log API health status
      const healthyApis = apiStats.filter(stats => stats.isHealthy).length
      this.updateProgress('Preparing', 20, '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ API...', 
        `üíö Healthy APIs: ${healthyApis}/${API_KEYS.length}`)

      await this.simulateDetailedProgress('summarize')
      
      const result = await this.callAI(audioFile, 'summarize')
      
      const processingTime = Date.now() - startTime
      this.updateProgress('Completed', 100, '‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!', 
        `üéâ Summary completed in ${processingTime}ms, Result length: ${result.length} characters`)

      return {
        success: true,
        data: result,
        processingTime: processingTime
      }
    } catch (error: any) {
      const processingTime = Date.now() - startTime
      this.updateProgress('Error', 0, '‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î', 
        `‚ùå Error after ${processingTime}ms: ${error.message}`)
      
      return {
        success: false,
        error: error.message
      }
    }
  }

  // Get performance stats
  getPerformanceStats() {
    return {
      totalApis: API_KEYS.length,
      stats: apiStats.map((stats, index) => ({
        apiIndex: index + 1,
        requests: stats.requests,
        errors: stats.errors,
        successRate: (stats.successRate * 100).toFixed(1) + '%',
        avgResponseTime: stats.avgResponseTime.toFixed(0) + 'ms',
        isHealthy: stats.isHealthy
      }))
    }
  }
}

export const googleAI = new GoogleAIService()
